{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.223951Z",
     "start_time": "2018-08-15T13:06:09.303454Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gtda.time_series import SlidingWindow\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "import sys\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import helpermethods\n",
    "from edgeml_tf.trainer.protoNNTrainer import ProtoNNTrainer\n",
    "from edgeml_tf.graph.protoNN import ProtoNN\n",
    "import edgeml_tf.utils as utils\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from scipy.stats import uniform\n",
    "from data_utils import *\n",
    "import re\n",
    "from mango.tuner import Tuner\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "from tensorflow import keras\n",
    "import tensorflow.compat.v1.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/home/nesl/earable_light/Activity_Dataset/' #Dataset directory\n",
    "model_dir = 'trained_models/'\n",
    "window_size = 550\n",
    "stride = 50\n",
    "channels = 2\n",
    "\n",
    "X_tr, Y_tr, X_test, Y_test = import_auritus_activity_dataset(dataset_folder = f, \n",
    "                                use_timestamp=False, \n",
    "                                shuffle=True, \n",
    "                                window_size = window_size, stride = stride, \n",
    "                                return_test_set = True, test_set_size = 300,channels=2)\n",
    "print(X_tr.shape)\n",
    "print(Y_tr.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_size = 10\n",
    "featX_tr = np.zeros((X_tr.shape[0],feat_size))\n",
    "featX_test = np.zeros((X_test.shape[0],feat_size))\n",
    "for i in range(X_tr.shape[0]):\n",
    "    cur_win = X_tr[i]\n",
    "    featX_tr[i,0] = np.min(cur_win[:,0])\n",
    "    featX_tr[i,1] = np.min(cur_win[:,1])\n",
    "    featX_tr[i,2] = np.max(cur_win[:,0])\n",
    "    featX_tr[i,3] = np.max(cur_win[:,1])\n",
    "    featX_tr[i,4] = featX_tr[i,2]-featX_tr[i,0]\n",
    "    featX_tr[i,5] = featX_tr[i,3]-featX_tr[i,1]\n",
    "    featX_tr[i,6] = np.var(cur_win[:,0])\n",
    "    featX_tr[i,7] = np.var(cur_win[:,1])\n",
    "    featX_tr[i,8] = np.sqrt(featX_tr[i,6])\n",
    "    featX_tr[i,9] = np.sqrt(featX_tr[i,7])  \n",
    "    \n",
    "for i in range(X_test.shape[0]):\n",
    "    cur_win = X_test[i]\n",
    "    featX_test[i,0] = np.min(cur_win[:,0])\n",
    "    featX_test[i,1] = np.min(cur_win[:,1])\n",
    "    featX_test[i,2] = np.max(cur_win[:,0])\n",
    "    featX_test[i,3] = np.max(cur_win[:,1])\n",
    "    featX_test[i,4] = featX_test[i,2]-featX_test[i,0]\n",
    "    featX_test[i,5] = featX_test[i,3]-featX_test[i,1]\n",
    "    featX_test[i,6] = np.var(cur_win[:,0])\n",
    "    featX_test[i,7] = np.var(cur_win[:,1])\n",
    "    featX_test[i,8] = np.sqrt(featX_test[i,6])\n",
    "    featX_test[i,9] = np.sqrt(featX_test[i,7])\n",
    "    \n",
    "x_train = featX_tr\n",
    "y_train = Y_tr\n",
    "x_test = featX_test\n",
    "y_test = Y_test\n",
    "numClasses = Y_tr.shape[1]\n",
    "dataDimension = x_train.shape[1]\n",
    "\n",
    "mean = np.mean(x_train, 0)\n",
    "std = np.std(x_train, 0)\n",
    "std[std[:] < 0.000001] = 1\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "Y_tr_int = np.argmax(Y_tr, axis=1)\n",
    "Y_test_int = np.argmax(Y_test,axis=1)\n",
    "\n",
    "shutil.rmtree('earable_dataset/', ignore_errors=True)\n",
    "os.mkdir('earable_dataset/')\n",
    "train = np.concatenate([Y_tr_int.reshape((Y_tr_int.shape[0],1)),x_train],axis=1)\n",
    "with open('earable_dataset/train.npy','wb') as f:\n",
    "    np.save(f,train)\n",
    "test = np.concatenate([Y_test_int.reshape((Y_test_int.shape[0],1)),x_test],axis=1)\n",
    "with open('earable_dataset/test.npy','wb') as f:\n",
    "    np.save(f,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3 #epochs to train each model for\n",
    "bayesEpochs = 2 #epochs for hyperparameter tuning\n",
    "log_file_name = 'Hyperparams_ProtoNN.csv'\n",
    "if os.path.exists(log_file_name):\n",
    "    os.remove(log_file_name)\n",
    "if os.path.exists(log_file_name[0:-4]+'.p'):\n",
    "    os.remove(log_file_name[0:-4]+'.p')\n",
    "row_write = ['score', 'accuracy','Flash','PROJECTION_DIM','NUM_PROTOTYPES','GAMMA']\n",
    "with open(log_file_name, 'a', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_NN(PROJECTION_DIM = 70, NUM_PROTOTYPES = 70, GAMMA = 0.004):\n",
    "    \n",
    "    #Fixed hyperparamters:\n",
    "    \n",
    "    REG_W = 0.000005\n",
    "    REG_B = 0.0\n",
    "    REG_Z = 0.00005\n",
    "    SPAR_W = 0.8\n",
    "    SPAR_B = 1.0\n",
    "    SPAR_Z = 1.0\n",
    "    LEARNING_RATE = 0.03\n",
    "    BATCH_SIZE =32    \n",
    "     \n",
    "    W, B, gamma = helpermethods.getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                       NUM_PROTOTYPES, x_train)\n",
    "    \n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "    protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                      NUM_PROTOTYPES, numClasses,\n",
    "                      gamma, W=W, B=B)\n",
    "    trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                             SPAR_W, SPAR_B, SPAR_Z,\n",
    "                             LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "    sess = tf.Session()\n",
    "    trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "                  printStep=600, valStep=1) \n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "    os.mkdir(model_dir)\n",
    "    acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "    W, B, Z, gamma = protoNN.getModelMatrices()\n",
    "    matrixList = sess.run([W, B, Z])\n",
    "    gamma = sess.run(gamma)\n",
    "    sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n",
    "    nnz, size, sparse = helpermethods.getModelSize(matrixList, sparcityList, expected=False)\n",
    "    np.save(model_dir + '/W.npy', matrixList[0])\n",
    "    np.save(model_dir + '/B.npy', matrixList[1])\n",
    "    np.save(model_dir + '/Z.npy', matrixList[2])\n",
    "    np.save(model_dir + '/gamma.npy', gamma)\n",
    "    \n",
    "    model_size = size #flash usage\n",
    "    accu = acc #accuracy\n",
    "    \n",
    "    score = 1.0*accu + 0.0*model_size #you can weigh the score to take into account model size too\n",
    "    row_write = [score, accu,model_size,PROJECTION_DIM,NUM_PROTOTYPES,GAMMA]\n",
    "    with open(log_file_name, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(row_write)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def save_res(data, file_name):\n",
    "    pickle.dump(data, open(file_name, \"wb\" ))\n",
    "    \n",
    "param_dict = {\n",
    "    'PROJECTION_DIM': np.arange(10,70),\n",
    "    'NUM_PROTOTYPES': np.arange(10,70),\n",
    "    'GAMMA': uniform(0.0015,0.05)\n",
    "}\n",
    "\n",
    "def objfunc(args_list):\n",
    "\n",
    "    objective_evaluated = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for hyper_par in args_list:\n",
    "        PROJECTION_DIM = hyper_par['PROJECTION_DIM']\n",
    "        NUM_PROTOTYPES = hyper_par['NUM_PROTOTYPES']\n",
    "        GAMMA = float(hyper_par['GAMMA'])\n",
    "            \n",
    "        objective = objective_NN(PROJECTION_DIM=PROJECTION_DIM,NUM_PROTOTYPES=NUM_PROTOTYPES,\n",
    "                                 GAMMA=GAMMA)\n",
    "        objective_evaluated.append(objective)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('objective:', objective, ' time:',end_time-start_time)\n",
    "        \n",
    "    return objective_evaluated\n",
    "\n",
    "conf_Dict = dict()\n",
    "conf_Dict['batch_size'] = 1 \n",
    "conf_Dict['num_iteration'] = bayesEpochs\n",
    "conf_Dict['initial_random']= 5\n",
    "tuner = Tuner(param_dict, objfunc,conf_Dict)\n",
    "all_runs = []\n",
    "results = tuner.maximize()\n",
    "all_runs.append(results)\n",
    "save_res(all_runs,log_file_name[0:-4]+'.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_DIM = results['best_params']['PROJECTION_DIM']\n",
    "NUM_PROTOTYPES = results['best_params']['NUM_PROTOTYPES']\n",
    "GAMMA = float(results['best_params']['GAMMA'])\n",
    "\n",
    "REG_W = 0.000005\n",
    "REG_B = 0.0\n",
    "REG_Z = 0.00005\n",
    "SPAR_W = 0.8\n",
    "SPAR_B = 1.0\n",
    "SPAR_Z = 1.0\n",
    "LEARNING_RATE = 0.03\n",
    "BATCH_SIZE =32    \n",
    "\n",
    "W, B, gamma = helpermethods.getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                   NUM_PROTOTYPES, x_train)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                  NUM_PROTOTYPES, numClasses,\n",
    "                  gamma, W=W, B=B)\n",
    "trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                         SPAR_W, SPAR_B, SPAR_Z,\n",
    "                         LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "sess = tf.Session()\n",
    "trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "              printStep=600, valStep=1) \n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "os.mkdir(model_dir)\n",
    "acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "W, B, Z, gamma = protoNN.getModelMatrices()\n",
    "matrixList = sess.run([W, B, Z])\n",
    "gamma = sess.run(gamma)\n",
    "sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n",
    "nnz, size, sparse = helpermethods.getModelSize(matrixList, sparcityList, expected=False)\n",
    "np.save(model_dir + '/W.npy', matrixList[0])\n",
    "np.save(model_dir + '/B.npy', matrixList[1])\n",
    "np.save(model_dir + '/Z.npy', matrixList[2])\n",
    "np.save(model_dir + '/gamma.npy', gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart Kernel before proceeding to prevent memory errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gtda.time_series import SlidingWindow\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "import sys\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import helpermethods\n",
    "from edgeml_tf.trainer.protoNNTrainer import ProtoNNTrainer\n",
    "from edgeml_tf.graph.protoNN import ProtoNN\n",
    "import edgeml_tf.utils as utils\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from scipy.stats import uniform\n",
    "from data_utils import *\n",
    "import re\n",
    "from mango.tuner import Tuner\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "from tensorflow import keras\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "\n",
    "model_dir = 'trained_models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved in 'trainedModels/'\n",
    "os.system(\"python3 protoNN_model_to_tflite.py --data-dir ./earable_dataset/ --output-dir \"+model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"xxd -i \"+model_dir+\"protoNN_model.tflite > model.cc\")\n",
    "with open('model.cc') as f:\n",
    "    z = f.readlines()\n",
    "f.close()   \n",
    "z.insert(0,'#include \"model.h\"\\n#ifdef __has_attribute\\n#define HAVE_ATTRIBUTE(x) __has_attribute(x)\\n#else\\n#define HAVE_ATTRIBUTE(x) 0\\n#endif\\n#if HAVE_ATTRIBUTE(aligned) || (defined(__GNUC__) && !defined(__clang__))\\n#define DATA_ALIGN_ATTRIBUTE __attribute__((aligned(4)))\\n#else\\n#define DATA_ALIGN_ATTRIBUTE\\n#endif\\n')\n",
    "z = [w.replace('trained_models_protoNN_model_tflite','protoNN_model') for w in z]\n",
    "z = [w.replace('unsigned char protoNN_model[]','const unsigned char protoNN_model[] DATA_ALIGN_ATTRIBUTE') for w in z]\n",
    "z[-1] = \"\"\n",
    "my_f = open(\"model.cc\",\"w\")\n",
    "for item in z:\n",
    "    my_f.write(item)\n",
    "my_f.close()\n",
    "\n",
    "\n",
    "h_file_cont = ['#ifndef PROTONN_MODEL_H_\\n',\n",
    "           '#define PROTONN_MODEL_H_\\n',\n",
    "          'extern const unsigned char protoNN_model[];\\n',\n",
    "          '#endif\\n']\n",
    "my_f = open(\"model.h\",\"w\")\n",
    "for item in h_file_cont:\n",
    "    my_f.write(item)\n",
    "my_f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
